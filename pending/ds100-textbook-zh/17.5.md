# 经验概率分布的近似

> 原文：[https://www.textbook.ds100.org/ch/17/classification_cost_justification.html](https://www.textbook.ds100.org/ch/17/classification_cost_justification.html)

```
# HIDDEN
# Clear previously defined variables
%reset -f

# Set directory for data loading to work properly
import os
os.chdir(os.path.expanduser('~/notebooks/17'))

```

```
# HIDDEN
import warnings
# Ignore numpy dtype warnings. These warnings are caused by an interaction
# between numpy and Cython and can be safely ignored.
# Reference: https://stackoverflow.com/a/40846742
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
%matplotlib inline
import ipywidgets as widgets
from ipywidgets import interact, interactive, fixed, interact_manual
import nbinteract as nbi

sns.set()
sns.set_context('talk')
np.set_printoptions(threshold=20, precision=2, suppress=True)
pd.options.display.max_rows = 7
pd.options.display.max_columns = 8
pd.set_option('precision', 2)
# This option stops scientific notation for pandas
# pd.set_option('display.float_format', '{:.2f}'.format)

```

在本节中，我们介绍了**kl 散度**，并演示了如何将二进制分类中的平均 kl 散度最小化等同于将平均交叉熵损失最小化。

由于逻辑回归输出概率，逻辑模型产生一定类型的概率分布。具体地说，基于最优参数$\hat \boldSymbol \theta$，它估计标签$y$对于示例输入$\textbf x 为$1$的可能性。

例如，假设$X$是一个标量，记录了一天的预测下雨机会，$Y=1$意味着 DOE 先生带着雨伞去工作。一个带有标量参数$\hat \theta$的逻辑模型预测，在预测下雨的可能性下，doe 先生带伞的概率为：$\hat p \theta（y=1 x）$。

收集有关 doe 先生伞式使用的数据为我们提供了一种构建经验概率分布$p（y=1_x）$的方法。例如，如果有五天下雨的可能性是$X=0.60 美元，而 DOE 先生只带了一把雨伞去上班，$P（Y=1_X=0.60）=0.20 美元。我们可以为数据中出现的每个$x$值计算类似的概率分布。当然，在拟合逻辑模型后，我们希望模型预测的分布尽可能接近数据集的经验分布。也就是说，对于我们数据中出现的所有$x$值，我们需要：

$$ \hat{P_\theta}(y = 1 | x) \approx P(y = 1 | x) $$

确定两个概率分布的“紧密性”的一个常用度量是 Kullback——Leibler 散度，或称 kl 散度，其根源在于信息论。

## 定义平均 kl 发散度[¶](#Defining-Average-KL-Divergence)

KL 散度量化了由我们的 Logistic 模型计算的概率分布$\Hat P BoldSymbol \Theta$与基于数据集的实际分布$P$之间的差异。直观地，它计算逻辑模型如何不精确地估计标签在数据中的分布。

对于单个数据点$（\textbf x，y）$的两个分布$p$和$\hat p boldsymbol \theta$之间的二进制分类的 kl 差异由以下公式给出：

$$D(P || \hat{P_\boldsymbol{\theta}}) = P(y = 0 | \textbf{x}) \ln \left(\frac{P(y = 0 | \textbf{x})}{\hat{P_\boldsymbol{\theta}}(y = 0 | \textbf{x})}\right) + P(y = 1 | \textbf{x}) \ln \left(\frac{P(y = 1 | \textbf{x})}{\hat{P_\boldsymbol{\theta}}(y = 1 | \textbf{x})}\right)$$

KL 散度不是对称的，即$p$与$p$与$p$与$p$与$p$与$p$与$d（P 124 124 \\124 \124 \123 \ \ \ \\123 \\\ \\\\\\\\\（P）$$

由于我们的目标是使用$\hat p boldsymbol \theta 美元，因此我们关注的是$d（p \hat p \boldsymbol \theta 美元）。

最好的$\BoldSymbol \Theta$值，我们将其表示为$\Hat \BoldSymbol \Theta$将整个$n$点数据集的平均 kl 发散最小化：

$$ \text{Average KL Divergence} = \frac{1}{n} \sum_{i=1}^{n} \left(P(y_i = 0 | \textbf{X}_i) \ln \left(\frac{P(y_i = 0 | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = 0 | \textbf{X}_i)}\right) + P(y_i = 1 | \textbf{X}_i) \ln \left(\frac{P(y_i = 1 | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = 1 | \textbf{X}_i)}\right)\right)$$$$ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\boldsymbol{\theta}}} (\text{Average KL Divergence}) $$

在上面的公式中，数据点$i ^ \text th 表示为（$\textbf x u i$，$y i$），其中，$\textbf x u i$是观察到的结果。

KL 的差异并不影响与$P$相关的罕见事件的不匹配。如果模型预测实际罕见事件的高概率，那么$p（k）$和$\ln\left（\frac p（k）p boldsymbol \theta（k）right）$都很低，因此差异也很低。但是，如果模型预测实际常见事件的概率较低，则散度较高。我们可以推断，与精确预测罕见事件但在常见事件上差异很大的模型相比，精确预测常见事件的逻辑模型与$P$的差异较小。

## 由 kl 发散推导交叉熵损失

上述平均 kl 散度方程的结构与交叉熵损失具有一些表面相似性。我们现在将用一些代数操作证明，最小化平均 kl 散度实际上等于最小化平均交叉熵损失。

Using properties of logarithms, we can rewrite the weighted log ratio: $$P(y_i = k | \textbf{X}_i) \ln \left(\frac{P(y_i = k | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = k | \textbf{X}_i)}\right) = P(y_i = k | \textbf{X}_i) \ln P(y_i = k | \textbf{X}_i) - P(y_i = k | \textbf{X}_i) \ln \hat{P_\粗体符号\ theta（y u i=k \ textbf x u i）$$

请注意，由于第一个术语不依赖于$\BoldSymbol \Theta$，因此它不会影响$\DisplayStyle\Arg\Min \Substack \BoldSymbol \Theta$并且可以从公式中删除。由此得到的表达式是模型的交叉熵损失$\Hat P BoldSymbol \Theta$：

$$ \text{Average Cross-Entropy Loss} = \frac{1}{n} \sum_{i=1}^{n} - P(y_i = 0 | \textbf{X}_i) \ln \hat{P_\theta}(y_i = 0 | \textbf{X}_i) - P(y_i = 1 | \textbf{X}_i) \ln \hat{P_\theta}(y_i = 1 | \textbf{X}_i)$$$$ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\theta}} (\text{Average Cross-Entropy Loss}) $$

由于标签$Y_I$是已知值，因此$Y_I=1$、$P（Y_I=1 textbf x u I）$等于$Y_I$和$P（Y_I=0 textbf x u I）$的概率等于$1-Y_I$。模型的概率分布$\hat p boldsymbol \theta 由前两节讨论的 sigmoid 函数的输出给出。进行这些替换后，我们得出平均交叉熵损失方程：

$$ \text{Average Cross-Entropy Loss} = \frac{1}{n} \sum_i \left(- y_i \ln (f_\hat{\boldsymbol{\theta}}(\textbf{X}_i)) - (1 - y_i) \ln (1 - f_\hat{\boldsymbol{\theta}}(\textbf{X}_i) \right) $$$$ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\theta}} (\text{Average Cross-Entropy Loss}) $$

## 交叉熵损失的统计解释

交叉熵损失在统计学上也有基础。由于逻辑回归模型预测概率，给定一个特定的逻辑模型，我们可以问，“这个模型产生一组观察到的结果的概率是多少？”我们可以自然地调整模型的参数，直到从模型中提取数据集的概率尽可能高。尽管在本节中我们不会证明这一点，但该程序相当于最小化交叉熵损失，这是交叉熵损失的 _ 最大似然 _ 统计证明。

## 摘要[¶](#Summary)

平均 kl 差异可以解释为$p$和$hat p boldsymbol \theta$p$加权的两个分布之间的平均对数差异。最小化平均 kl 发散也最小化平均交叉熵损失。我们可以通过选择对常见数据进行精确分类的参数来减少逻辑回归模型的分歧。